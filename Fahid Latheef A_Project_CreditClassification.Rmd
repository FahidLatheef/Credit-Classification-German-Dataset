---
title: "Credit Scoring Model"
author: "Fahid Latheef A"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  prettydoc::html_pretty:
    theme: leonids
    highlight: github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, warning=FALSE, echo = FALSE,message=FALSE}
#install.packages("ggpubr")
#install.packages("rpart")
library(gmodels)
library(rpart)
library(rpart.plot)
library(rattle)
library(knitr)
library(dplyr)
library(tidyr)
library(reshape2)
library(ggplot2)
library(scales)
library(ggrepel)
library(ggpubr) #For ggarrange - arranging multiple graphs
```

## Introduction

 In this project, I analysed German Credit Dataset and try to make a model to classify potentially good or bad customer. This document is created using R-Markdown. The .rmd file is attached submitted along with the pdf file. Moreover, some of the R codes are deliberately hidden from the pdf file to avoid being repetitive which can be referred from the RMD file. 

 The criteria that I am using to compare the models is Penalty per Data measurement which is equal to total Penalty / No. of Datapoints. Here I am using **Confusion Matrix** to evaluate the total penalty. 
 
 Assume a person is a bad customer, that is he will default his credit. If we predict him to be a good customer it is a huge loss compared to the opportunity cost you are missing while predicting a good customer to be a bad customer. So the penalty given for former case (False Negative in defaulting) should be higher than the later (False Positive) Case. Here, in this project, I am using a Penalty ratio of 3:1 so that the model is strict in giving loan and the Default rates are minimized.

## Brief on the Variables

Let's first load the German credit dataset into R and check what the variables are.

```{r}
# Reading the dataset
credit <- read.csv("C:\\Users\\Fahid\\Desktop\\AFM\\AFM-2\\Class 3\\Credit Dataset.csv")

# Finding the Variables
names(credit)

```

There are 21 variables in the dataset. I will try to explain the variables in the dataset (After individiually examining the variables).

**default**: Binary Target variable. It says whether the applicant has defaulted or not. 0 means he didn't default and 1 means he does. 

**Account.Balance** : Qualitative variable with 4 categories. It explains about the current status of their existing check account. The categories are self explanatory, the currency used is DM ( Deutsche Mark - German Currency): \
*0 DM* \
*1-200 DM* \
*\> 200 DM* \
*unknown* - No checking account \

**Duration.of.Credit..month** : Numerical variable. Explains about the duration in months of credit taken

**Payment.Status.of.Previous.Credit** : Categorical variables. There are 5 categories: \
*critical* -  Critical account, Other credits existing in other banks \
*delayed* - Delay in paying off in the past \
*fully repaid* - Existing credits paid back duly till now \
*fully repaid this bank* - All credits at this bank paid back duly \
*repaid* - No credits taken or all credits paid back duly \

**Purpose** : Categorical variable. Purpose of the credit. 10 categories such as business, car (new and used), education, furniture, radio/tv, repair etc.

**Credit.Amount** : Numerical variable which gives the credit amount

**Value.Savings.Stocks** : Categorical variable which tells about the savings account. 5 categories \
*< 100 DM* \
*\> 1000 DM* \
*101 - 500 DM* \
*501 - 1000 DM* \
*unknown* - No savings account/unknown status \

**Length.of.current.employment** : Categorical variable which tells about the Present employment experience length. Categories are *0 - 1 yrs*,  *1 - 4 yrs*, *4 - 7 yrs*, *\> 7 yrs* and *unemployed* \

**Instalment.per.cent** : Numerical variable. Installment rate in percentage terms.

**Sex...Marital.Status** : Categorical variables. 4 categories which are *divorced male*, *female*, *married male* and *single male*.

**Guarantors** : Categorical variable which tells about the other debtors/guarantors. 3 categories which are *co-applicant*, *guarantor* and *none*.

**Duration.in.Current.address** Numerical variable which tells about the years of residency in the present address.

**Most.valuable.available.asset** Categorical variable which categorizes the applicant's most valuable asset. Categories are *building society savings*, *other*, *real estate* and *unknown/none*.

**Age..years** : Numerical variable which tells about the age of the applicant.

**Concurrent.Credits** : Categorical variable with 3 categories *bank*, *stores* and *none* which tells about other installment plans of the applicant.

**Type.of.apartment** : Categorical variable with 3 categories *for free*, *own* and *rent* which alk about the applicant's housing.

**No.of.Credits.at.this.Bank** : Numerical variable which tells about the number of existing credits at this bank.

**No.of.dependents** : Numerical variable which tells about the number of people being liable to provide maintenance for the applicant. More the dependence, safer the applicant.

**Telephone** : Binary variable which tells whether the applicant's phone number registered under his name or not.

**Foreign.Worker** Binary variable which tells whether the worker is foreign or not.

**Job** : Qualitative variable which tells about type of Job the applicant has. 4 categories which are *mangement self-employed*, *skilled employee*, *unemployed non-resident* and *unskilled resident*.

## Exploratory Data Analysis and Variable Transformation

Let's dive straight into Exploratory Data analysis.

```{r}
# Dataset Exploration
dim(credit)
```

There are 21 variables and 1000 rows which talks about 1000 applicants with the above explained variables. Let's check whether there is any missing data or not.

```{r}
# Find missing values in the entire dataframe
sum(is.na(credit))
```

As you can see there aren't any missing values or NA values. Now, let's observe the structure of every variable in the dataset to see whether some of them can be converted (transformed) from continuous to categorical or vice-versa. 

```{r}
sapply(credit, class,simplify = TRUE)
```

One thing we can observe from the structure of the variables and the brief of the variables topic is that few of the variables are not structured correctly. The continuous variables `Instalment.per.cent`, `Duration.in.Current.address`, `No.of.Credits.at.this.Bank` and `No.of.dependents` are currently in integer structure and it will be better to convert them to factor/categorical format for better exploration and modelling. 
```{r}
names <- c(9,12,17,18)
credit[,names] <- lapply(credit[,names] , factor)
```

Using the indices of the variables I transformed these variables to factors. Let's crosscheck now.

```{r}
sapply(credit, class,simplify = TRUE)
```

The required variable transformation is done. Now, let's continue with our data exploration. Now, I will try to summarize each variable.
```{r}
summary(credit)
```


Before progressing further, let's split the data to train and test data set in 70% - 30% ratio.

```{r}
# Total number of rows in the credit data frame
n <- nrow(credit)

# Number of rows for the training set (70% of the dataset)
n_train <- round(0.7 * n) 

# Train-Test-Split
# Create a vector of indices which is an 70% random sample
set.seed(259879)
train_indices <- sample(1:n, size = n_train, replace = FALSE)

# Subset the credit data frame to training indices only
credit_train <- credit[train_indices, ]  

# Exclude the training indices to create the test set
credit_test <- credit[-train_indices, ]  
```

Let's assume that we are only given this train data and we have to build a model on it. Assume that the test data contains the new applicant's data which will be used to test our model's strength.

Let's try to explore the train data further.

```{r}
t1 <- data.frame(table(credit_train$default))
names(t1) <- c("default", "Freq")
t1 <- t1 %>% mutate(Percent = Freq/sum(t1$Freq))
g1 <- ggplot(t1, aes(x = default, y = Freq)) +
  geom_bar(stat = "identity", width = 0.2)+
  geom_text(aes(label = Freq), color ="yellow", vjust = 1.5,
        size = 5)+
  geom_text(aes(label = paste0(round(100 * Percent, 1), "%")), color ="white", 
        vjust = 5.5, size = 5)+
  labs(y="Frequency and Percentage ", x = "default (1 - defaulted, 0 - did not default) ")+
  ggtitle("Number and Percentage of Defaults")
plot(g1)
  

```

```{r,echo =FALSE}
t2 <- data.frame(table(credit_train$Account.Balance, credit_train$default))

names(t2) <- c('DMcategory', 'default', 'Freq')
g2 <- ggplot(data=t2, aes(x=DMcategory, y=Freq, fill=default)) +
  geom_bar(stat="identity") + ylab(" ") + ggtitle("Default rate vs Account Balance")+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

t3 <- data.frame(table(credit_train$Payment.Status.of.Previous.Credit,
                       credit_train$default))

names(t3) <- c('Paymentstatus', 'default', 'Freq')

g3 <- ggplot(data=t3, aes(x=Paymentstatus, y=Freq, fill=default)) +
  geom_bar(stat="identity") + ylab(" ") + ggtitle("Default rate vs Payment Status")+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

ggarrange(g2,g3, common.legend = TRUE)
```

```{r}
d <- select(credit_train, 1,3,6)
d <- d %>% mutate(default_cat = ifelse(default == 1,"Default","No Default"))

ggplot(d, aes(x=Duration.of.Credit..month., y=Credit.Amount, color=default_cat)) +
  geom_point() + ggtitle("Credit Amount and Duration vs Default")
```

```{r,echo =FALSE}
t4 <- data.frame(table(credit_train$Purpose, credit_train$default))

names(t4) <- c('Purpose', 'default', 'Freq')

g4<- ggplot(data=t4, aes(x=Purpose, y=Freq, fill=default)) +
  geom_bar(stat="identity") + ylab(" ") +
  theme(axis.text.x = element_text(angle = 50, hjust = 1))+
  ggtitle("Default rate vs Purpose of credit")
  

t5 <- data.frame(table(credit_train$Value.Savings.Stocks, credit_train$default))

names(t5) <- c('SavingsAct', 'default', 'Freq')

g5 <- ggplot(data=t5, aes(x=SavingsAct, y=Freq, fill=default)) +
  geom_bar(stat="identity") + ylab(" ") + xlab("Savings Account Category")+
  ggtitle("Default rate vs Savings Account")+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

ggarrange(g4,g5, common.legend = TRUE)

```


```{r, echo = FALSE}
t6 <- data.frame(table(credit_train$Length.of.current.employment, credit_train$default))

names(t6) <- c('WorkExp', 'default', 'Freq')

g6<- ggplot(data=t6, aes(x=WorkExp, y=Freq, fill=default)) +
  geom_bar(stat="identity") + ylab(" ") + xlab("Current Work Experience")+
  ggtitle("Default rate vs Current Work Ex")+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
  

t7 <- data.frame(table(credit_train$Instalment.per.cent, credit_train$default))

names(t7) <- c('Instalpercent', 'default', 'Freq')

g7 <- ggplot(data=t7, aes(x=Instalpercent, y=Freq, fill=default)) +
  geom_bar(stat="identity") + ylab(" ") + xlab("Installment Rate")+
  ggtitle("Installment Rate")

ggarrange(g6,g7,common.legend = TRUE)
```


```{r,echo = FALSE}
t8 <- data.frame(table(credit_train$Guarantors, credit_train$default))

names(t8) <- c('Guarantors', 'default', 'Freq')

g8<- ggplot(data=t8, aes(x=Guarantors, y=Freq, fill=default)) +
  geom_bar(stat="identity") + ylab(" ") + xlab("Guarantors")+
  ggtitle("Default rate vs Guarantors")
  

t9 <- data.frame(table(credit_train$No.of.dependents, credit_train$default))

names(t9) <- c('No.of.dependents', 'default', 'Freq')

g9 <- ggplot(data=t9, aes(x=No.of.dependents, y=Freq, fill=default)) +
  geom_bar(stat="identity") + ylab(" ") + xlab("No. of dependents")+
  ggtitle("Defaut rate vs No. of dependents")

ggarrange(g8,g9,common.legend = TRUE)
```


```{r, echo=FALSE}
t10 <- data.frame(table(credit_train$Concurrent.Credits, credit_train$default))

names(t10) <- c('Other.Credits', 'default', 'Freq')

g10<- ggplot(data=t10, aes(x=Other.Credits, y=Freq, fill=default)) +
  geom_bar(stat="identity") + ylab(" ") + xlab("Other Credits")+
  ggtitle("Default rate vs Other Credits")
  

t11 <- data.frame(table(credit_train$No.of.Credits.at.this.Bank, credit_train$default))

names(t11) <- c('No.of.Credits.at.this.bank', 'default', 'Freq')

g11<- ggplot(data=t11, aes(x=No.of.Credits.at.this.bank, y=Freq, fill=default)) +
  geom_bar(stat="identity") + ylab(" ") + xlab("No. of Credits at this bank")+
  ggtitle("Defaut rate vs No. of credits")

ggarrange(g10,g11,common.legend = TRUE)
```

```{r,echo = FALSE}
t10 <- data.frame(table(credit_train$Concurrent.Credits, credit_train$default))

names(t10) <- c('Other.Credits', 'default', 'Freq')

g10<- ggplot(data=t10, aes(x=Other.Credits, y=Freq, fill=default)) +
  geom_bar(stat="identity") + ylab(" ") + xlab("Other Credits")+
  ggtitle("Default rate vs Other Credits")
  

t11 <- data.frame(table(credit_train$No.of.Credits.at.this.Bank, credit_train$default))

names(t11) <- c('No.of.Credits.at.this.bank', 'default', 'Freq')

g11<- ggplot(data=t11, aes(x=No.of.Credits.at.this.bank, y=Freq, fill=default)) +
  geom_bar(stat="identity") + ylab(" ") + xlab("No. of Credits at this bank")+
  ggtitle("Defaut rate vs No. of credits")

ggarrange(g10,g11,common.legend = TRUE)


```
```{r}
CrossTable(credit_train$Account.Balance, credit_train$default, prop.r = T, prop.c = T, 
           prop.t = T, chisq = T, prop.chisq = T)

CrossTable(credit_train$Payment.Status.of.Previous.Credit, credit_train$default,
           prop.r = T, 
           prop.c = T, prop.t = T, chisq = F, prop.chisq = F)
CrossTable(credit_train$Purpose,credit_train$default, digits=1, prop.r=F, prop.t=F,
           prop.chisq=F, chisq=T)
```

The chi-square helps us in deciding how much variance is explained by each variable on the target variable. This helps us in deciding which all variables are the most important.
### Variable Transformation

Using Weight of Evidence and Information Value we can transform the variable (Similar to Normalizing) to improve our model. This also helps in explaining the customer/manager why some particular credit was rejected/ approved. One of the other use of this is to deal with the outliers

## Base model run using Logistic regression
### Model 1 - Base Model
Let's run the logistic model with all the variables included.

```{r}

model_base_logistic <- glm(default~.,data = credit_train, family = "binomial")
summary(model_base_logistic)
```

We can see the summary of the model and different levels of significance for each variable categories. Now, Let's try to find the Training model strength by comparing actual and predicted `default` values. 

```{r, echo = FALSE}
predictTrain = predict(model_base_logistic, type="response")
cat("\n Summary of predicted probabilities \n\n")
summary(predictTrain)
cat("\n Average Probability for good and bad credit \n\n")
# Average prediction probabilities
tapply(predictTrain, credit_train$default, mean)
```

In the above table we can see that the predicted probability for the Training set vary between 0 and 1 with the mean value of 0.294. Moreover, the average predicted probability from the base model for the good accounts (`default`=0) is 0.19 whereas for bad accounts (`default`=1) it is 0.54.

Let's build the confusion matrix for the same with a cut-off of 0.5.
```{r}
# Confusion matrix for threshold of 0.5

cat("Confusion Matrix for 0.5 cut-off level \n")
table(credit_train$default, ifelse(predictTrain > 0.5,1,0))
```

The predicted class are the columns and the actual class are the rows.

Clearly this classification has an accuracy rate of (437+120)/700 = 79.57%  for the Training set. We can try different set of cut-offs between the average predicted probability for good and bad account (In this base model, between 0.19 and 0.54)

We are trying to detect whether an account will default (1) or not(0). As mentioned in the introductory part of the report, we are using the penalty-confusion matrix method to decide the cut-off percentage. If our model predicts an account as a good account and it is actually a default/bad account (False Negative) a penalty of 3 DM and for a wrongly predicting a account to be bad account (False Positive) an opportunity cost of 1 DM is given. This helps the model to improve its prediction ability to predict good accounts.

Let's use a for loop to implement this.

```{r}
for (i in 19:54)
{
falsepos <- table(credit_train$default, ifelse(predictTrain > i/100,1,0))[1,2] 
falseneg <- table(credit_train$default, ifelse(predictTrain > i/100,1,0))[2,1]
penalty  <- falsepos*1 + falseneg*3
print(paste("Penalty for",i,"% cut-off is",penalty))
}
```

The lowest penalty is at 28% cut-off (235 penalty). And penalty per data is 235/700 = 0.3357. Let's use this as the final base model cut-off. Confusion matrix for the same is given below

```{r,echo =FALSE}
cat("Confusion Matrix for cut-off level 0.28 for Training Dataset \n")
table(credit_train$default, ifelse(predictTrain > 0.28,1,0))
```

This has an accuracy rate of (379 + 166)/700 = 77.86% which is lower than 50% cut-off accuracy rate (79.57%), but it helps in minimizing false negatives.

Now, let's use this base model and cut-off percentage to see the error rate of the test data.

```{r,echo =FALSE}
predictTest = predict(model_base_logistic, type="response", newdata = credit_test)
cat("Confusion Matrix for cut-off level 0.28 for Testing Dataset \n")
table(credit_test$default, ifelse(predictTest >= 0.28,1,0))
```

Our model has an accuracy rate of (138+60)/300 = 66% with the Testing Dataset and has a penalty of 3\*34 + 1\*68 = 170 DM. And Penalty per data is 170/300 = 0.5667 which is higher than the train data as expected.

### Model 2 - Updated logistic model with only significant variables at 10% level

From the base model summary we were able to see the significantce level of each coefficient. I am using 10% significance level to further reduce the model variables so that we can follow the *principle of parsimony* : Simpler the model, the better. From the base model, 10 of the variables are significant at 10% level. Let's try this model with the glm function.
```{r,echo =FALSE}

model_significant_logistic <- glm(default~ Account.Balance +
                                    Duration.of.Credit..month. +
                                    Payment.Status.of.Previous.Credit +
                                    Purpose+
                                    Credit.Amount +
                                    Value.Savings.Stocks +
                                    Length.of.current.employment +
                                    Instalment.per.cent +
                                    Duration.in.Current.address +
                                    Foreign.Worker, 
                                  data = credit_train, family = "binomial")
summary(model_significant_logistic)
```

We can see the summary of the model and different levels of significance for each variable categories. Now, Let's try to find the Training model strength by comparing actual and predicted `default` values. 

```{r, echo = FALSE}
predictTrain2 = predict(model_significant_logistic, type="response")
cat("\n Summary of predicted probabilities \n\n")
summary(predictTrain2)
cat("\n Average Probability for good and bad credit \n\n")
# Average prediction probabilities
tapply(predictTrain2, credit_train$default, mean)
```

In the above table we can see that the predicted probability for the Training set vary between 0 and 1 with the mean value of 0.294. Moreover, the average predicted probability from the base model for the good accounts (`default`=0) is 0.20 whereas for bad accounts (`default`=1) it is 0.52.

Let's try to find the best cut-off for this model using the for loop.

```{r,echo =FALSE}
for (i in 20:52)
{
falsepos <- table(credit_train$default, ifelse(predictTrain2 > i/100,1,0))[1,2] 
falseneg <- table(credit_train$default, ifelse(predictTrain2 > i/100,1,0))[2,1]
penalty  <- falsepos*1 + falseneg*3
print(paste("Penalty for",i,"% cut-off is",penalty))
}
```

At 23% and 25% we have the least penalty of 246 and penalty per data is 246/700 = 0.351. To decide among the cutoff percentage let's calculate the accuracy rate of the classification for both these cut-offs and then take the model with better accuracy.

```{r,echo =FALSE}
# Confusion matrix for threshold of 0.23

cat("Confusion Matrix for 0.23 cut-off level Training Dataset \n")
table(credit_train$default, ifelse(predictTrain2 > 0.23,1,0))
```

Accuracy rate = (335+177)/700 = 73.142

```{r,echo =FALSE}
# Confusion matrix for threshold of 0.25

cat("Confusion Matrix for 0.25 cut-off level Training Dataset \n")
table(credit_train$default, ifelse(predictTrain2 > 0.25,1,0))
```

Accuracy rate = (353+171)/700 = 74.857

The cut-off level 25% gives better accuracy, so let's chose 25% as the cut-off. Now, let's use this second model and 25% cut-off level to see the error rate of the test data.

```{r,echo =FALSE}
predictTest2 = predict(model_significant_logistic, type="response", newdata = credit_test)
cat("Confusion Matrix for cut-off level 0.25 for Testing Dataset with Model 2 \n")
table(credit_test$default, ifelse(predictTest2 >= 0.25,1,0))
```

Our model has an accuracy rate of (132+65)/300 = 65.66% with the Testing Dataset and has a penalty of 3\*29 + 1\*74 = 161 DM. And Penalty per data is 161/300 = 0.5367 which is higher than the train data penalty per data (0.351) as expected. Moreover, the penalty per data for model 1 (base model) was 0.5667 and hence this model 2 is deinitely an improvement on the Model 1 (Base Model).


## Segmentation

### Model 3 - Fully grown Decision tree

Let's allow the Decision Tree to be fully grown. Let's define the penalty matrix first based on the false positives and false negatives.

```{r}
lossmatrix <- matrix(c(0,3,1,0), byrow = TRUE, nrow = 2)
lossmatrix
```

Let's use this penalty matrix to penalize each misclassifications and use rpart to grow the Decision Tree.

```{r}
set.seed(259879)
credittree <- rpart(default ~. , data = credit_train,
                    method = "class",parms = list(loss = lossmatrix))
fancyRpartPlot(credittree)
```

Let's see the variable importance percentage for this fully grown decision tree.

```{r,echo=FALSE}
cat("Fully Grown Decision Tree Variable Importance in Percentage \n")
credittree$variable.importance
```

Let's see the Training Dataset model strength for this fully grown Decision Tree (a deep tree) with the confusion matrix.

```{r}
pred = predict(credittree, type="class")
conf.matrix <- table(pred, credit_train$default)
rownames(conf.matrix) <- c("Actual good credit (0)", "Actual bad  credit (1)")
colnames(conf.matrix) <- c("Predicted good credit (0)", "Predicted bad  credit (1)")
cat("Confusion Matrix for Training Dataset with Model 3 - Fully grown Decision Tree \n")
conf.matrix
```

The accuracy rate of Training Dataset for Model 3 is (488 + 67)/700 = 79.29%. The penalty for this model is 139\*1 + 6\*3 = 157. penalty per data is 157/700 = 0.2242. This result for Training Dataset is a great improvement in penalty per data from Model 1 and Model 2.

Now let's try predicting test data with this model.
```{r,echo =FALSE}

predtest = predict(credittree, credit_test[-1], type="class")
conf.matrix <- table(predtest, credit_test$default)
rownames(conf.matrix) <- c("Actual good credit (0)", "Actual bad  credit (1)")
colnames(conf.matrix) <- c("Predicted good credit (0)", "Predicted bad  credit (1)")
cat("Confusion Matrix for Testing Dataset with Model 3 - Fully grown Decision Tree \n")
conf.matrix

```

The accuracy rate of Testing Dataset for Model 3 is (190 + 13)/700 = 67.66%. The penalty for this model is 81\*1 + 16\*3 = 129. penalty per data is 129/300 = 0.43. As expected this penalty per data is worse than the training dataset. However, this measure is far better than penalty per data for test dataset for Model 1 or Model 2, which is hence an improvement.

For the model 3, penalty per data for Training data is 0.22 and 0.43 for Testing Data. It is because of the overfitting in the fully grown tree. Let's prune the tree to improve the results.
Here the best pruned tree is found by iterating the CP (complexity parameter) to find the CP which gives the best reults. The code given below gives us the best pruned tree. Also I am putting a minsplit = 200, minbucket = 100 as parameters so that the categories are not too small.
```{r,echo =FALSE}

ptree <- rpart(default ~. , data = credit_train, control =
                 rpart.control(minsplit = 200, minbucket =100,  cp=credittree$cptable[which.min(credittree$cptable[,"xerror"]),"CP"]),
               parms = list(loss = lossmatrix))
fancyRpartPlot(ptree)

```

Using this pruned Tree, we can run seperate models in each category for classification.


## Model Summary in a Table

```{r,echo =FALSE}
model_1 <- c("28%",235,0.336,"77.86%",170,0.5667,"66.00%")
model_2 <- c("25%",246,0.351,"73.14%",161,0.5367,"65.66%" )
model_3 <- c(NaN, 157, 0.224,"79.29%",129,0.4300,"67.66%")
models <- rbind(model_1,model_2,model_3)
colnames(models) <- c("Cut-off","Train_Penalty","Train_Penalty_per_data","Train_Accuracy","Test_Penalty","Test_Penalty_per_data","Test_Accuracy" )
t(models)

```

### Comaprison of the Models

The Decision Tree (Model 3) is the best model for classifying German Credit Dataset. For generalizing Model 3 is the best followed by Model 2 and Model 1 respectively. The generalizing ability (Test) may be further improved by pruning the Decision Tree.

### Creating a credit-score from the log(odds)

The credit score can be easily created by scaling the log(odds) value between our measurement (say 300 to 900). So credit score closer to 900 will have low probability of defaulting while credit scores closer to 300 will have huge probability of default. The credit score range can be anything we wish it to be.


